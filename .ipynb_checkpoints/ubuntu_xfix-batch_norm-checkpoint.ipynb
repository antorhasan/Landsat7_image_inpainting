{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "K0JU1wEzOH07",
    "outputId": "ec3065cd-da37-40ce-8763-cda85a3a6cf8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import gc\n",
    "#import cv2\n",
    "#import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")    #for tensorboard\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "agRyZL61OH1P"
   },
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    \n",
    "        \n",
    "        \n",
    "    features = {\n",
    "                \"image_y\": tf.FixedLenFeature((), tf.string ),\n",
    "                \"image_m\": tf.FixedLenFeature((), tf.string )\n",
    "                #\"image_x\": tf.FixedLenFeature((), tf.string )\n",
    "                }\n",
    "\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "\n",
    "    image_y = tf.decode_raw(parsed_features[\"image_y\"],  tf.float64)\n",
    "    image_m = tf.decode_raw(parsed_features[\"image_m\"],  tf.float64)\n",
    "\n",
    "    image_y = tf.reshape(image_y, [256,256,1])\n",
    "    image_m = tf.reshape(image_m, [256,256,1])\n",
    "    #image_x = tf.decode_raw(parsed_features[\"image_x\"],  tf.float64)\n",
    "    #tf.summary.image(\"64_Y\",image_y,3)\n",
    "    #tf.summary.image(\"64_M\",image_m,3)\n",
    "\n",
    "    image_y = tf.cast(image_y,dtype=tf.float32)\n",
    "    image_m = tf.cast(image_m,dtype=tf.float32)\n",
    "    #image_x = tf.cast(image_x,dtype=tf.float32)\n",
    "    #tf.summary.image(\"32_Y\",image_y,3)\n",
    "    #tf.summary.image(\"32_M\",image_m,3)\n",
    "\n",
    "    #image_y = tf.reshape(image_y, [512,512,1])\n",
    "    #image_m = tf.reshape(image_m, [512,512,1])\n",
    "    #image_x = tf.reshape(image_x, [512,512,1])\n",
    "\n",
    "    return image_y,image_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(inputs, is_training, decay=.5, epsilon=0.00000001):\n",
    "    with tf.name_scope(\"batch_norm\") as scope:\n",
    "\n",
    "\n",
    "        scale = tf.get_variable(\"scale_BN\", (inputs.get_shape()[1:4]), initializer=tf.ones_initializer())\n",
    "        beta = tf.get_variable(\"beta_BN\", (inputs.get_shape()[1:4]), initializer=tf.zeros_initializer())\n",
    "        pop_mean = tf.get_variable(\"pop_mean\", (inputs.get_shape()[1:4]), initializer=tf.zeros_initializer(), trainable=False)\n",
    "        pop_var = tf.get_variable(\"pop_var\", (inputs.get_shape()[1:4]), initializer=tf.ones_initializer(), trainable=False)\n",
    "\n",
    "        mean = tf.cond(tf.cast(is_training,tf.bool), lambda: tf.nn.moments(inputs,[0])[0], lambda: tf.multiply(tf.ones(inputs.get_shape()[1:4]), pop_mean))\n",
    "        var = tf.cond(tf.cast(is_training,tf.bool), lambda: tf.nn.moments(inputs,[0])[1], lambda: tf.multiply(tf.ones(inputs.get_shape()[-1]), pop_var))\n",
    "        train_mean = tf.cond(tf.cast(is_training,tf.bool), lambda:tf.assign(pop_mean, pop_mean*decay+mean*(1-decay)),lambda:tf.zeros(1))\n",
    "        train_var = tf.cond(tf.cast(is_training,tf.bool),lambda:tf.assign(pop_var, pop_var*decay+var*(1-decay)),lambda:tf.zeros(1))\n",
    "\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs, mean, var, beta, scale, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZZqe6faCOH1R"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def partial_conv(pixel, mask,is_training, kernel_size, filter_numbers, stride, batch_n, nonlinearity, trans):\n",
    "\n",
    "    with tf.name_scope(\"part_conv\") as scope:\n",
    "        kernel_h = kernel_size[0]\n",
    "        kernel_w = kernel_size[1]\n",
    "        if trans==True:\n",
    "            kernel_d = filter_numbers\n",
    "            kernel_o = pixel.get_shape().as_list()[3]\n",
    "        elif trans==False:\n",
    "            kernel_d = pixel.get_shape().as_list()[3]\n",
    "            kernel_o = filter_numbers\n",
    "        elif trans==\"same_pad\":\n",
    "            #kernel_d = pixel.get_shape().as_list()[3]\n",
    "            #kernel_o = filter_numbers\n",
    "            kernel_d = filter_numbers\n",
    "            kernel_o = pixel.get_shape().as_list()[3]\n",
    "        elif trans==\"one\":\n",
    "            kernel_d = pixel.get_shape().as_list()[3]\n",
    "            kernel_o = filter_numbers\n",
    "            \n",
    "            \n",
    "        W = tf.get_variable('Weights', (kernel_h, kernel_w, kernel_d, kernel_o),\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        \n",
    "        \n",
    "        W1 = tf.ones((kernel_h, kernel_w, kernel_d, kernel_o), name='Weights_mask')\n",
    "\n",
    "        Z1 = tf.multiply(pixel, mask, name=\"element_op\")\n",
    "\n",
    "        if trans==True:\n",
    "            #need to fix for variable last batch size. The last mini_batch will be of different size most of the time\n",
    "            out_shape_list = pixel.get_shape().as_list()\n",
    "            out_shape_list[1] = pixel.get_shape().as_list()[1] + 2\n",
    "            out_shape_list[2] = pixel.get_shape().as_list()[2] + 2\n",
    "            out_shape_list[3] = filter_numbers\n",
    "            out_shape = tf.constant(out_shape_list)\n",
    "            #out_shape = tf.TensorShape(out_shape_list)\n",
    "            #out_shape = tf.cast(out_shape,tf.int32)\n",
    "            prime_conv = tf.nn.conv2d_transpose(Z1, W,out_shape, strides=stride, padding=\"VALID\", name=\"prime_conv\")\n",
    "            sec_conv = tf.nn.conv2d_transpose(mask, W1,output_shape=tf.TensorShape(out_shape_list), strides=stride, padding=\"VALID\", name=\"sec_conv\")\n",
    "        elif trans==False:\n",
    "            prime_conv = tf.nn.conv2d(Z1, W, strides=stride, padding=\"VALID\", name=\"prime_conv\")\n",
    "            sec_conv = tf.nn.conv2d(mask, W1, strides=stride, padding=\"VALID\", name=\"sec_conv\")\n",
    "        elif trans==\"same_pad\":\n",
    "            #prime_conv = tf.nn.conv2d(Z1, W, strides=stride, padding=\"SAME\", name=\"prime_conv\")\n",
    "            #sec_conv = tf.nn.conv2d(mask, W1, strides=stride, padding=\"SAME\", name=\"sec_conv\")\n",
    "            out_shape_list = pixel.get_shape().as_list()\n",
    "            out_shape_list[1] = pixel.get_shape().as_list()[1] \n",
    "            out_shape_list[2] = pixel.get_shape().as_list()[2] \n",
    "            out_shape_list[3] = filter_numbers\n",
    "            out_shape = tf.constant(out_shape_list)\n",
    "            prime_conv = tf.nn.conv2d_transpose(Z1, W,out_shape, strides=stride, padding=\"SAME\", name=\"prime_conv\")\n",
    "            sec_conv = tf.nn.conv2d_transpose(mask, W1,output_shape=tf.TensorShape(out_shape_list), strides=stride, padding=\"SAME\", name=\"sec_conv\")\n",
    "        elif trans==\"one\":\n",
    "            prime_conv = tf.nn.conv2d(Z1, W, strides=stride, padding=\"VALID\", name=\"prime_conv\")\n",
    "            sec_conv = tf.nn.conv2d(mask, W1, strides=stride, padding=\"VALID\", name=\"sec_conv\")\n",
    "            \n",
    "\n",
    "        inver_sum = tf.divide(tf.constant(1.0), sec_conv)\n",
    "        clean_sum = tf.where(tf.is_inf(inver_sum), tf.zeros_like(inver_sum), inver_sum)\n",
    "\n",
    "        weighted_pixel = tf.multiply(prime_conv, clean_sum, name=\"multi_inver_sum\")\n",
    "        up_mask = tf.where(tf.not_equal(sec_conv, tf.constant(0.0)),tf.ones_like(sec_conv),sec_conv)\n",
    "\n",
    "        #normalized_out = tf.cond(tf.cast(batch_n,tf.bool), lambda:batch_norm(weighted_pixel, tf.cast(is_training,tf.bool)), lambda:weighted_pixel)\n",
    "        #B = tf.get_variable('Biases',(1,weighted_pixel.get_shape()[1],weighted_pixel.get_shape()[2],weighted_pixel.get_shape()[3]),\n",
    "        #                    initializer=tf.constant_initializer(.01))\n",
    "        B = tf.get_variable('Biases',(1,1,1,prime_conv.get_shape()[3]),\n",
    "                            initializer=tf.constant_initializer(.01))\n",
    "        \n",
    "        normalized_out = tf.add(weighted_pixel,B)\n",
    "        #normalized_out = weighted_pixel\n",
    "        \n",
    "        if nonlinearity==\"relu\":\n",
    "            up_pixel = tf.nn.relu(normalized_out, name=\"relu\")\n",
    "        elif nonlinearity==\"leaky_relu\":\n",
    "            up_pixel = tf.nn.leaky_relu(normalized_out, name=\"leaky_relu\")\n",
    "        elif nonlinearity==\"none\":\n",
    "            #up_pixel = normalized_out\n",
    "            up_pixel = tf.sigmoid(normalized_out)\n",
    "            #up_pixel = tf.nn.relu(normalized_out, name=\"relu\")\n",
    "        elif nonlinearity==\"elu\":\n",
    "            up_pixel = tf.keras.activations.elu(normalized_out)\n",
    "            \n",
    "        tf.summary.histogram(\"weights\", W)    \n",
    "        tf.summary.histogram(\"biases\", B)   \n",
    "        tf.summary.histogram(\"activations\", up_pixel)   \n",
    "        \n",
    "        return up_pixel, up_mask\n",
    "    \n",
    "\n",
    "\n",
    "def place_holders(mini_size,height, width, channels):\n",
    "    #X = tf.placeholder(tf.float32, shape=(mini_size, height, width, channels))\n",
    "    Y = tf.placeholder(tf.float32, shape=(mini_size, height, width, channels))\n",
    "    M = tf.placeholder(tf.float32, shape=(mini_size, height, width, channels))\n",
    "    return M ,Y\n",
    "\n",
    "\n",
    "def near_up_sampling(pixel, mask, output_size):\n",
    "    with tf.name_scope(\"nearest_up\") as scope:\n",
    "        up_pixel = tf.image.resize_nearest_neighbor(pixel, size=output_size, name=\"nearest_pixel_up\")\n",
    "        up_mask = tf.image.resize_nearest_neighbor(pixel, size=output_size, name=\"nearest_mask_up\")\n",
    "        return up_pixel, up_mask\n",
    "\n",
    "def concat(near_pixel, pconv_pixel, near_mask, pconv_mask):\n",
    "    with tf.name_scope(\"concatenation\") as scope:\n",
    "        up_pixel = tf.concat([pconv_pixel, near_pixel], axis=3)\n",
    "        up_mask = tf.concat([pconv_mask,near_mask], axis=3)\n",
    "        return up_pixel, up_mask\n",
    "\n",
    "def decoding_layer(pixel_in,mask_in,is_training, output_size_in, pconv_pixel1, pconv_mask1, filter_numbers1):\n",
    "    with tf.name_scope(\"decoding\") as scope:\n",
    "        near_pixel1,near_mask1 = near_up_sampling(pixel_in,mask_in,output_size_in)\n",
    "        concat_pixel,concat_mask = concat(near_pixel1, pconv_pixel1, near_mask1, pconv_mask1)\n",
    "        pixel_out,mask_out = partial_conv(concat_pixel,concat_mask,is_training,[3,3],filter_numbers1,[1,1,1,1],\n",
    "                                        True,\"leaky_relu\",trans=True)\n",
    "        return pixel_out,mask_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mFUVWDrVOH1V"
   },
   "outputs": [],
   "source": [
    "def forward_prop(is_training, pixel, mask):\n",
    "    non_lin = \"relu\"\n",
    "    \n",
    "#     with tf.variable_scope(\"PConv1\") as scope:\n",
    "#         p_out1,m_out1 = partial_conv(pixel,mask,is_training,kernel_size=[3,3],filter_numbers=64,stride=[1,2,2,1],\n",
    "#                                     batch_n=False,nonlinearity=\"relu\",trans=False)\n",
    "    with tf.variable_scope(\"PConv2\") as scope:\n",
    "        p_out2,m_out2 = partial_conv(pixel,mask,is_training,kernel_size=[3,3],filter_numbers=64,stride=[1,2,2,1],\n",
    "                                    batch_n=True,nonlinearity=non_lin,trans=False)\n",
    "    with tf.variable_scope(\"PConv3\") as scope:\n",
    "        p_out3,m_out3 = partial_conv(p_out2,m_out2,is_training,kernel_size=[3,3],filter_numbers=128,stride=[1,2,2,1],\n",
    "                                    batch_n=True,nonlinearity=non_lin,trans=False)\n",
    "    with tf.variable_scope(\"PConv4\") as scope:\n",
    "        p_out4,m_out4 = partial_conv(p_out3,m_out3,is_training,kernel_size=[3,3],filter_numbers=256,stride=[1,2,2,1],\n",
    "                                    batch_n=True,nonlinearity=non_lin,trans=False)\n",
    "    with tf.variable_scope(\"PConv5\") as scope:\n",
    "        p_out5,m_out5 = partial_conv(p_out4,m_out4,is_training,kernel_size=[3,3],filter_numbers=256,stride=[1,2,2,1],\n",
    "                                    batch_n=True,nonlinearity=non_lin,trans=False)\n",
    "    with tf.variable_scope(\"PConv6\") as scope:\n",
    "        p_out6,m_out6 = partial_conv(p_out5,m_out5,is_training,kernel_size=[3,3],filter_numbers=512,stride=[1,2,2,1],\n",
    "                                    batch_n=True,nonlinearity=non_lin,trans=False)\n",
    "    with tf.variable_scope(\"PConv7\") as scope:\n",
    "        p_out7,m_out7 = partial_conv(p_out6,m_out6,is_training,kernel_size=[3,3],filter_numbers=512,stride=[1,2,2,1],\n",
    "                                    batch_n=True,nonlinearity=non_lin,trans=False)\n",
    "    with tf.variable_scope(\"PConv8\") as scope:\n",
    "        p_out8,m_out8 = partial_conv(p_out7,m_out7,is_training,kernel_size=[3,3],filter_numbers=512,stride=[1,1,1,1],\n",
    "                                    batch_n=True,nonlinearity=non_lin,trans=False)\n",
    "    with tf.variable_scope(\"decoding9\") as scope:\n",
    "        p_out9,m_out9 = decoding_layer(p_out8,m_out8,is_training,(p_out7.get_shape().as_list()[1],p_out7.get_shape().as_list()[2]),\n",
    "                                        p_out7,m_out7,filter_numbers1=512)\n",
    "\n",
    "    with tf.variable_scope(\"decoding10\") as scope:\n",
    "        p_out10,m_out10 = decoding_layer(p_out9,m_out9,is_training,(p_out6.get_shape().as_list()[1],p_out6.get_shape().as_list()[2]),\n",
    "                                        p_out6,m_out6,filter_numbers1=512)\n",
    "\n",
    "    with tf.variable_scope(\"decoding11\") as scope:\n",
    "        p_out11,m_out11 = decoding_layer(p_out10,m_out10,is_training,(p_out5.get_shape().as_list()[1],p_out5.get_shape().as_list()[2]),\n",
    "                                        p_out5,m_out5,filter_numbers1=256)\n",
    "\n",
    "    with tf.variable_scope(\"decoding12\") as scope:\n",
    "        p_out12,m_out12 = decoding_layer(p_out11,m_out11,is_training,(p_out4.get_shape().as_list()[1],p_out4.get_shape().as_list()[2]),\n",
    "                                        p_out4,m_out4,filter_numbers1=256)\n",
    "\n",
    "    with tf.variable_scope(\"decoding13\") as scope:\n",
    "        p_out13,m_out13 = decoding_layer(p_out12,m_out12,is_training,(p_out3.get_shape().as_list()[1],p_out3.get_shape().as_list()[2]),\n",
    "                                        p_out3,m_out3,filter_numbers1=128)\n",
    "\n",
    "    with tf.variable_scope(\"decoding14\") as scope:\n",
    "        p_out14,m_out14 = decoding_layer(p_out13,m_out13,is_training,(p_out2.get_shape().as_list()[1],p_out2.get_shape().as_list()[2]),\n",
    "                                        p_out2,m_out2,filter_numbers1=64)\n",
    "\n",
    "#     with tf.variable_scope(\"decoding15\") as scope:\n",
    "#         p_out15,m_out15 = decoding_layer(p_out14,m_out14,is_training,(p_out1.get_shape().as_list()[1],p_out1.get_shape().as_list()[2]),\n",
    "#                                         p_out1,m_out1,filter_numbers1=64)\n",
    "\n",
    "    #with tf.variable_scope(\"decoding16\") as scope:\n",
    "    #    p_out16,m_out16 = decoding_layer(p_out15,m_out15,is_training,(pixel.get_shape().as_list()[1],pixel.get_shape().as_list()[2]),\n",
    "    #                                    pixel,mask,filter_numbers1=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.variable_scope(\"decoding15\") as scope:\n",
    "        near_pixel1,near_mask1 = near_up_sampling(p_out14,m_out14,(pixel.get_shape().as_list()[1],pixel.get_shape().as_list()[2]))\n",
    "        pixel_hole = tf.multiply(pixel, mask, name=\"multiply_mask\")\n",
    "        concat_pixel,concat_mask = concat(near_pixel1, pixel_hole, near_mask1, mask)\n",
    "        pixel_out,mask_out = partial_conv(concat_pixel,concat_mask,is_training,[1,1],filter_numbers=1,stride=[1,1,1,1],\n",
    "                                        batch_n=False,nonlinearity=\"none\",trans=\"one\")\n",
    "    \n",
    "    return pixel_out,mask_out\n",
    "\n",
    "\n",
    "\n",
    "def compute_cost(pixel_gt,mask_gt,pixel_pre,hole_pera,valid_pera):\n",
    "    with tf.name_scope(\"cost\") as scope:\n",
    "        loss_valid = tf.losses.absolute_difference(tf.multiply(pixel_gt,mask_gt),tf.multiply(pixel_pre,mask_gt), weights=1.0,\n",
    "                                                   reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS)\n",
    "        loss_hole = tf.losses.absolute_difference(tf.multiply(pixel_gt,(1-mask_gt)),tf.multiply(pixel_pre,(1-mask_gt)), weights=1.0,\n",
    "                                                    reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS)\n",
    "        \n",
    "        #loss_hole = loss_hole * loss_hole\n",
    "        #loss_valid = loss_valid * loss_valid \n",
    "        #loss_valid = tf.losses.mean_squared_error(tf.multiply(pixel_gt,mask_gt),tf.multiply(pixel_pre,mask_gt))\n",
    "        #loss_hole = tf.losses.mean_squared_error(tf.multiply(pixel_gt,(1-mask_gt)),tf.multiply(pixel_pre,(1-mask_gt)))\n",
    "\n",
    "        #total_loss = (tf.multiply(valid_pera,loss_valid) + tf.multiply(hole_pera,loss_hole))/(hole_pera+valid_pera)\n",
    "        #total_loss = (loss_valid + tf.multiply(hole_pera,loss_hole))/(hole_pera)\n",
    "        total_loss = (valid_pera*loss_valid + hole_pera*loss_hole)/(hole_pera+valid_pera)\n",
    "\n",
    "        tf.summary.scalar('loss',total_loss)\n",
    "   \n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lgvAO8mGOH1d"
   },
   "outputs": [],
   "source": [
    "def model(learning_rate,num_epochs,mini_size,break_t,break_v,pt_out,hole_pera,valid_pera):\n",
    "    #ops.reset_default_graph()\n",
    "    tf.summary.scalar('learning_rate',learning_rate)\n",
    "    tf.summary.scalar('batch_size',mini_size)\n",
    "    tf.summary.scalar('training_break',break_t)\n",
    "    tf.summary.scalar('validation_break',break_v)\n",
    "    tf.summary.scalar('print_interval',pt_out)\n",
    "    tf.summary.scalar('hole_loss_weight',hole_pera)\n",
    "    tf.summary.scalar('valid_loss_weight',valid_pera)\n",
    "  \n",
    "    m = 19488\n",
    "    #m = 8\n",
    "    #h = 512\n",
    "    #w = 512\n",
    "    #c = 1\n",
    "    \n",
    "    m_val_size = 1888\n",
    "        \n",
    "    #filenames = \"/media/antor/Files/ML/Papers/train_mfix.tfrecords\"\n",
    "    filenames = tf.placeholder(tf.string)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.shuffle(200)\n",
    "    dataset = dataset.batch(mini_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    " \n",
    "    pix_gt, mask_in = iterator.get_next()\n",
    "    \n",
    "    pix_gt = tf.reshape(pix_gt,[mini_size,256,256,1])\n",
    "    mask_in = tf.reshape(mask_in,[mini_size,256,256,1])\n",
    "    \n",
    "    tf.summary.image(\"input_Y\",pix_gt,3)\n",
    "    tf.summary.image(\"input_M\",mask_in,3)\n",
    "    \n",
    "    pixel_out, mask_out = forward_prop(is_training=is_training,pixel=pix_gt, mask=mask_in)\n",
    "    \n",
    "    tf.summary.image(\"output_Y\",pixel_out,3)\n",
    "    tf.summary.image(\"output_M\",mask_out,3)\n",
    "    \n",
    "    cost = compute_cost(pixel_gt=pix_gt, mask_gt=mask_in, pixel_pre=pixel_out, hole_pera=hole_pera,valid_pera=valid_pera)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    num_mini = int(m/mini_size)          #must keep this fully divided and num_mini output as int pretty sure it doesn't need\n",
    "                                    #to be an int    \n",
    "    merge_sum = tf.summary.merge_all()\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())   #for tensorboard\n",
    "    \n",
    "    #saver = tf.train.Saver()    #for model saving\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    \n",
    "    sess.run(init)\n",
    "    sess.run(iterator.initializer,feed_dict={filenames:\"/media/antor/Files/ML/Papers/train_last.tfrecords\"})\n",
    "    \n",
    "    mini_cost = 0.0\n",
    "    counter = 1\n",
    "    epoch_cost = 0.0\n",
    "    epoch = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            _ , temp_cost = sess.run([optimizer,cost], feed_dict={is_training:True})\n",
    "                       \n",
    "            #mini_cost += temp_cost/num_mini\n",
    "            mini_cost += temp_cost/pt_out\n",
    "            epoch_cost += temp_cost/num_mini\n",
    "            \n",
    "            if counter%20 == 0:\n",
    "                s = sess.run(merge_sum)\n",
    "                file_writer.add_summary(s,counter)\n",
    "                            \n",
    "            if counter%num_mini==0:\n",
    "                print(\"cost after epoch \" + str(counter/num_mini) + \": \" + str(epoch_cost))\n",
    "                epoch_cost =0.0 \n",
    "                epoch+=1\n",
    "                \n",
    "            #print(\"cost after epoch \" + str(counter/num_mini) + \": \" + str(mini_cost))\n",
    "            \n",
    "            #if counter%1==0:\n",
    "            #    print(\"mini batch cost of batch \" + str(counter) + \" is : \" + str(temp_cost))\n",
    "            \n",
    "            if counter%pt_out==0:\n",
    "                print(\"mini batch cost of batch \" + str(counter) + \" is : \" + str(mini_cost))\n",
    "                mini_cost =0.0 \n",
    "                #gc.collect()\n",
    "                \n",
    "            #if counter*mini_size>=break_t:\n",
    "            #    break\n",
    "            \n",
    "            if epoch ==  num_epochs:\n",
    "                break\n",
    "            \n",
    "            counter = counter + 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    \n",
    "               #for tensorboard\n",
    "    \n",
    "    num_mini_val = int(m_val_size/mini_size)\n",
    "\n",
    "    counter_val = 1\n",
    "\n",
    "    sess.run(iterator.initializer,feed_dict={filenames:\"/media/antor/Files/ML/Papers/val_last.tfrecords\"})\n",
    "    #sess.run(iterator_val.initializer,feed_dict={filenames_val:\"/media/antor/Files/ML/Papers/val_mfix.tfrecords\"})\n",
    "\n",
    "    mini_cost_val = 0.0\n",
    "    epoch_cost_val = 0.0\n",
    "    mini_br = int(break_v/mini_size)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            #temp_cost_val = sess.run(cost, feed_dict={M:mask_in_val,Y:label_in_val,is_training:False})\n",
    "            temp_cost_val = sess.run(cost, feed_dict={is_training:False})\n",
    "            #temp_cost_val = sess.run(cost_val)\n",
    "\n",
    "            epoch_cost_val += temp_cost_val/num_mini_val\n",
    "            mini_cost_val +=  temp_cost_val/mini_br\n",
    "            \n",
    "            if counter_val%num_mini_val==0:\n",
    "                print(\"cost after epoch : \" + str(epoch_cost_val))\n",
    "                tf.summary.scalar('total_cost',epoch_cost_val)\n",
    "                #epoch_cost_val =0.0 \n",
    "            \n",
    "            #if counter_val*mini_size>=break_v:\n",
    "            #    print(\"cost of val set : \" + str(mini_cost_val))\n",
    "            #    tf.summary.scalar('total_cost',mini_cost_val)\n",
    "            #    break\n",
    "            \n",
    "            \n",
    "            counter_val = counter_val + 1          \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    file_writer.close()    \n",
    "    #save_path = saver.save(sess, \"/home/antor/Downloads/model_checkpoint/my_model_final.ckpt\")\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from tensorflow.python.framework import ops\\n\\nf = np.random.uniform(-5,0,6)\\ni = 10**f\\n#print(f)\\nprint(i)\\nb = [2,4,8,16]\\nfor j in i:\\n    for k in b:\\n        print(j,k)\\n        model(learning_rate=j,num_epochs=1,mini_size=k,break_t=1500,break_v=150,pt_out=20,hole_pera=6.0,valid_pera=1.0)\\n        ops.reset_default_graph() '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from tensorflow.python.framework import ops\n",
    "\n",
    "f = np.random.uniform(-5,0,6)\n",
    "i = 10**f\n",
    "#print(f)\n",
    "print(i)\n",
    "b = [2,4,8,16]\n",
    "for j in i:\n",
    "    for k in b:\n",
    "        print(j,k)\n",
    "        model(learning_rate=j,num_epochs=1,mini_size=k,break_t=1500,break_v=150,pt_out=20,hole_pera=6.0,valid_pera=1.0)\n",
    "        ops.reset_default_graph() '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#i=.05\\nfrom tensorflow.python.framework import ops\\n#import random\\nh = np.random.randint(3,9,size=10)\\nv = np.random.randint(1,20,size=15)\\n#random.shuffle(h)\\n#random.shuffle(v)\\n\\n#f = np.random.uniform(np.log10(.0094958),np.log10(.009907),7)\\nf = np.random.uniform(.0094958,.009907,7)\\ni = 10**f\\n#i= [.07,.01,.007,.099999,.001]\\n#i = np.random.uniform(.01039,.04,5)\\nprint(i)\\nprint(f)\\n\\nfor k in f:\\n    \\n    print(k)\\n    model(learning_rate=k,num_epochs=1,mini_size=16,break_t=8000,break_v=100,pt_out=20,hole_pera=6.0,valid_pera=1.0)\\n    ops.reset_default_graph() '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#i=.05\n",
    "from tensorflow.python.framework import ops\n",
    "#import random\n",
    "h = np.random.randint(3,9,size=10)\n",
    "v = np.random.randint(1,20,size=15)\n",
    "#random.shuffle(h)\n",
    "#random.shuffle(v)\n",
    "\n",
    "#f = np.random.uniform(np.log10(.0094958),np.log10(.009907),7)\n",
    "f = np.random.uniform(.0094958,.009907,7)\n",
    "i = 10**f\n",
    "#i= [.07,.01,.007,.099999,.001]\n",
    "#i = np.random.uniform(.01039,.04,5)\n",
    "print(i)\n",
    "print(f)\n",
    "\n",
    "for k in f:\n",
    "    \n",
    "    print(k)\n",
    "    model(learning_rate=k,num_epochs=1,mini_size=16,break_t=8000,break_v=100,pt_out=20,hole_pera=6.0,valid_pera=1.0)\n",
    "    ops.reset_default_graph() '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "i5w7yUdWOH1h",
    "outputId": "a3dd9e34-f947-4e9c-be22-f5437697dd5f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini batch cost of batch 20 is : 0.05842430517077447\n",
      "mini batch cost of batch 40 is : 0.05620686504989863\n",
      "mini batch cost of batch 60 is : 0.0518239639699459\n",
      "mini batch cost of batch 80 is : 0.04866097550839186\n",
      "mini batch cost of batch 100 is : 0.05181872304528952\n",
      "mini batch cost of batch 120 is : 0.05635735094547273\n",
      "mini batch cost of batch 140 is : 0.053833436220884316\n",
      "mini batch cost of batch 160 is : 0.051074597239494306\n",
      "mini batch cost of batch 180 is : 0.05434170383960008\n",
      "mini batch cost of batch 200 is : 0.054325123876333224\n",
      "mini batch cost of batch 220 is : 0.05280272047966717\n",
      "mini batch cost of batch 240 is : 0.05248506162315607\n",
      "mini batch cost of batch 260 is : 0.05328760668635369\n",
      "mini batch cost of batch 280 is : 0.05345768146216869\n",
      "mini batch cost of batch 300 is : 0.05333586893975735\n",
      "mini batch cost of batch 320 is : 0.052660997956991196\n",
      "mini batch cost of batch 340 is : 0.05290060993283986\n",
      "mini batch cost of batch 360 is : 0.052603043802082534\n",
      "mini batch cost of batch 380 is : 0.0513903371989727\n",
      "mini batch cost of batch 400 is : 0.053307386487722395\n",
      "mini batch cost of batch 420 is : 0.05185528304427861\n",
      "mini batch cost of batch 440 is : 0.046913604438304904\n",
      "mini batch cost of batch 460 is : 0.04111422579735518\n",
      "mini batch cost of batch 480 is : 0.037364080920815466\n",
      "mini batch cost of batch 500 is : 0.03559616561979055\n",
      "mini batch cost of batch 520 is : 0.03327890504151584\n",
      "mini batch cost of batch 540 is : 0.03244624342769384\n",
      "mini batch cost of batch 560 is : 0.03243628023192287\n",
      "mini batch cost of batch 580 is : 0.031017714738845823\n",
      "mini batch cost of batch 600 is : 0.02944560442119836\n",
      "mini batch cost of batch 620 is : 0.02903000609949231\n",
      "mini batch cost of batch 640 is : 0.02617957834154367\n",
      "mini batch cost of batch 660 is : 0.024700017180293795\n",
      "mini batch cost of batch 680 is : 0.025760897342115643\n",
      "mini batch cost of batch 700 is : 0.024845824576914307\n",
      "mini batch cost of batch 720 is : 0.023813939932733775\n",
      "mini batch cost of batch 740 is : 0.024439642764627928\n",
      "mini batch cost of batch 760 is : 0.02409293428063393\n",
      "mini batch cost of batch 780 is : 0.023200869467109444\n",
      "mini batch cost of batch 800 is : 0.024626834690570826\n",
      "mini batch cost of batch 820 is : 0.02315374137833715\n",
      "mini batch cost of batch 840 is : 0.024160339124500756\n",
      "mini batch cost of batch 860 is : 0.023929925076663496\n",
      "mini batch cost of batch 880 is : 0.023135465197265146\n",
      "mini batch cost of batch 900 is : 0.02401285516098142\n",
      "mini batch cost of batch 920 is : 0.024847795907408\n",
      "mini batch cost of batch 940 is : 0.023764158692210914\n",
      "mini batch cost of batch 960 is : 0.023455591499805452\n",
      "mini batch cost of batch 980 is : 0.023389521706849336\n",
      "mini batch cost of batch 1000 is : 0.024373430106788874\n",
      "mini batch cost of batch 1020 is : 0.024427825305610898\n",
      "mini batch cost of batch 1040 is : 0.022737607918679712\n",
      "mini batch cost of batch 1060 is : 0.02374797975644469\n",
      "mini batch cost of batch 1080 is : 0.023574373032897708\n",
      "mini batch cost of batch 1100 is : 0.02330763414502144\n",
      "mini batch cost of batch 1120 is : 0.02321791276335716\n",
      "mini batch cost of batch 1140 is : 0.02320448150858283\n",
      "mini batch cost of batch 1160 is : 0.02329499516636133\n",
      "mini batch cost of batch 1180 is : 0.02487323833629489\n",
      "mini batch cost of batch 1200 is : 0.025959290843456988\n",
      "cost after epoch 1.0: 0.03591576626765952\n",
      "mini batch cost of batch 1220 is : 0.02573102172464132\n",
      "mini batch cost of batch 1240 is : 0.028797176294028764\n",
      "mini batch cost of batch 1260 is : 0.03984597455710173\n",
      "mini batch cost of batch 1280 is : 0.029067668039351697\n",
      "mini batch cost of batch 1300 is : 0.025220233481377367\n",
      "mini batch cost of batch 1320 is : 0.024493825528770684\n",
      "mini batch cost of batch 1340 is : 0.023985592368990186\n",
      "mini batch cost of batch 1360 is : 0.02470245882868767\n",
      "mini batch cost of batch 1380 is : 0.024493560567498213\n",
      "mini batch cost of batch 1400 is : 0.02366655375808478\n",
      "mini batch cost of batch 1420 is : 0.022877067793160677\n",
      "mini batch cost of batch 1440 is : 0.023312623519450425\n",
      "mini batch cost of batch 1460 is : 0.024033624865114695\n",
      "mini batch cost of batch 1480 is : 0.02355356998741627\n",
      "mini batch cost of batch 1500 is : 0.023159000743180516\n",
      "mini batch cost of batch 1520 is : 0.02265245532616973\n",
      "mini batch cost of batch 1540 is : 0.023253919370472432\n",
      "mini batch cost of batch 1560 is : 0.0222781646065414\n",
      "mini batch cost of batch 1580 is : 0.02320150900632143\n",
      "mini batch cost of batch 1600 is : 0.02257771100848913\n",
      "mini batch cost of batch 1620 is : 0.02257157135754824\n",
      "mini batch cost of batch 1640 is : 0.02254019184038043\n",
      "mini batch cost of batch 1660 is : 0.022635044902563094\n",
      "mini batch cost of batch 1680 is : 0.022202814649790525\n",
      "mini batch cost of batch 1700 is : 0.021458430029451848\n",
      "mini batch cost of batch 1720 is : 0.021538968663662676\n",
      "mini batch cost of batch 1740 is : 0.021911208238452676\n",
      "mini batch cost of batch 1760 is : 0.02333738850429654\n",
      "mini batch cost of batch 1780 is : 0.021895723603665825\n",
      "mini batch cost of batch 1800 is : 0.02243603859096766\n",
      "mini batch cost of batch 1820 is : 0.022103471681475636\n",
      "mini batch cost of batch 1840 is : 0.022905003093183038\n",
      "mini batch cost of batch 1860 is : 0.021957544889301066\n",
      "mini batch cost of batch 1880 is : 0.02235541203990579\n",
      "mini batch cost of batch 1900 is : 0.022032411396503443\n",
      "mini batch cost of batch 1920 is : 0.021177031937986607\n",
      "mini batch cost of batch 1940 is : 0.0215215103700757\n",
      "mini batch cost of batch 1960 is : 0.022066305205225945\n",
      "mini batch cost of batch 1980 is : 0.021225773077458144\n",
      "mini batch cost of batch 2000 is : 0.022874402906745674\n",
      "mini batch cost of batch 2020 is : 0.020464238384738563\n",
      "mini batch cost of batch 2040 is : 0.022379897907376288\n",
      "mini batch cost of batch 2060 is : 0.022004173696041105\n",
      "mini batch cost of batch 2080 is : 0.022876307927072046\n",
      "mini batch cost of batch 2100 is : 0.022432051319628953\n",
      "mini batch cost of batch 2120 is : 0.020311262831091883\n",
      "mini batch cost of batch 2140 is : 0.022070651967078445\n",
      "mini batch cost of batch 2160 is : 0.021947384718805547\n",
      "mini batch cost of batch 2180 is : 0.022530569881200786\n",
      "mini batch cost of batch 2200 is : 0.020864436309784653\n",
      "mini batch cost of batch 2220 is : 0.02127489857375622\n",
      "mini batch cost of batch 2240 is : 0.021626112330704928\n",
      "mini batch cost of batch 2260 is : 0.02122153788805008\n",
      "mini batch cost of batch 2280 is : 0.0206103015691042\n",
      "mini batch cost of batch 2300 is : 0.020950467046350243\n",
      "mini batch cost of batch 2320 is : 0.0211983080022037\n",
      "mini batch cost of batch 2340 is : 0.020701236743479963\n",
      "mini batch cost of batch 2360 is : 0.02223351448774337\n",
      "mini batch cost of batch 2380 is : 0.021446311660110948\n",
      "mini batch cost of batch 2400 is : 0.021479006670415406\n",
      "mini batch cost of batch 2420 is : 0.020943520311266183\n",
      "cost after epoch 2.0: 0.022801336487495192\n",
      "mini batch cost of batch 2440 is : 0.020980923622846606\n",
      "mini batch cost of batch 2460 is : 0.021324228309094907\n",
      "mini batch cost of batch 2480 is : 0.021046393550932407\n",
      "mini batch cost of batch 2500 is : 0.020824157074093817\n",
      "mini batch cost of batch 2520 is : 0.020620400644838808\n",
      "mini batch cost of batch 2540 is : 0.021106221433728933\n",
      "mini batch cost of batch 2560 is : 0.021340251434594393\n",
      "mini batch cost of batch 2580 is : 0.021051009744405748\n",
      "mini batch cost of batch 2600 is : 0.020705510489642618\n",
      "mini batch cost of batch 2620 is : 0.021164783183485265\n",
      "mini batch cost of batch 2640 is : 0.021048726513981823\n",
      "mini batch cost of batch 2660 is : 0.02156942365691066\n",
      "mini batch cost of batch 2680 is : 0.02060779565945268\n",
      "mini batch cost of batch 2700 is : 0.021204177010804418\n",
      "mini batch cost of batch 2720 is : 0.020304422918707134\n",
      "mini batch cost of batch 2740 is : 0.021242879191413516\n",
      "mini batch cost of batch 2760 is : 0.0205836396664381\n",
      "mini batch cost of batch 2780 is : 0.021604331489652392\n",
      "mini batch cost of batch 2800 is : 0.020147975441068416\n",
      "mini batch cost of batch 2820 is : 0.020816107653081416\n",
      "mini batch cost of batch 2840 is : 0.02051270781084895\n",
      "mini batch cost of batch 2860 is : 0.020048247510567305\n",
      "mini batch cost of batch 2880 is : 0.020273697655647994\n",
      "mini batch cost of batch 2900 is : 0.020851978007704015\n",
      "mini batch cost of batch 2920 is : 0.02090349970385432\n",
      "mini batch cost of batch 2940 is : 0.021850026305764905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini batch cost of batch 2960 is : 0.020020838268101217\n",
      "mini batch cost of batch 2980 is : 0.020915816072374582\n",
      "mini batch cost of batch 3000 is : 0.02199305985122919\n",
      "mini batch cost of batch 3020 is : 0.021157969441264864\n",
      "mini batch cost of batch 3040 is : 0.020334094297140833\n",
      "mini batch cost of batch 3060 is : 0.020537714567035438\n",
      "mini batch cost of batch 3080 is : 0.021726503828540446\n",
      "mini batch cost of batch 3100 is : 0.020413006469607354\n",
      "mini batch cost of batch 3120 is : 0.02087430227547884\n",
      "mini batch cost of batch 3140 is : 0.020266202650964262\n",
      "mini batch cost of batch 3160 is : 0.02079026885330677\n",
      "mini batch cost of batch 3180 is : 0.02101092091761529\n",
      "mini batch cost of batch 3200 is : 0.020362885016947982\n",
      "mini batch cost of batch 3220 is : 0.021166021376848226\n",
      "mini batch cost of batch 3240 is : 0.021208968572318552\n",
      "mini batch cost of batch 3260 is : 0.020624271128326654\n",
      "mini batch cost of batch 3280 is : 0.020680897263810036\n",
      "mini batch cost of batch 3300 is : 0.020774986967444422\n",
      "mini batch cost of batch 3320 is : 0.01982791675254703\n",
      "mini batch cost of batch 3340 is : 0.020989598892629146\n",
      "mini batch cost of batch 3360 is : 0.020006439741700882\n",
      "mini batch cost of batch 3380 is : 0.01928232773207128\n",
      "mini batch cost of batch 3400 is : 0.019144824519753457\n",
      "mini batch cost of batch 3420 is : 0.018449840694665907\n",
      "mini batch cost of batch 3440 is : 0.018775705061852933\n",
      "mini batch cost of batch 3460 is : 0.01798560926690698\n",
      "mini batch cost of batch 3480 is : 0.017225668020546436\n",
      "mini batch cost of batch 3500 is : 0.017530668154358862\n",
      "mini batch cost of batch 3520 is : 0.01756394300609827\n",
      "mini batch cost of batch 3540 is : 0.018083909992128614\n",
      "mini batch cost of batch 3560 is : 0.017605668772011994\n",
      "mini batch cost of batch 3580 is : 0.01720476374030113\n",
      "mini batch cost of batch 3600 is : 0.01756276851519942\n",
      "mini batch cost of batch 3620 is : 0.017148574627935887\n",
      "mini batch cost of batch 3640 is : 0.017074284004047514\n",
      "cost after epoch 3.0: 0.020123410993676195\n",
      "mini batch cost of batch 3660 is : 0.017502201441675424\n",
      "mini batch cost of batch 3680 is : 0.017372369952499866\n",
      "mini batch cost of batch 3700 is : 0.01699904832057655\n",
      "mini batch cost of batch 3720 is : 0.017182249668985607\n",
      "mini batch cost of batch 3740 is : 0.017376554105430847\n",
      "mini batch cost of batch 3760 is : 0.017270471388474106\n",
      "mini batch cost of batch 3780 is : 0.017295411136001346\n",
      "mini batch cost of batch 3800 is : 0.016841996926814318\n",
      "mini batch cost of batch 3820 is : 0.017612482514232395\n",
      "mini batch cost of batch 3840 is : 0.0170458463486284\n",
      "mini batch cost of batch 3860 is : 0.017017430486157534\n",
      "mini batch cost of batch 3880 is : 0.017034862330183392\n",
      "mini batch cost of batch 3900 is : 0.017363933520391582\n",
      "mini batch cost of batch 3920 is : 0.016888363054022193\n",
      "mini batch cost of batch 3940 is : 0.017255903547629712\n",
      "mini batch cost of batch 3960 is : 0.017628680029883978\n",
      "mini batch cost of batch 3980 is : 0.016499435575678945\n",
      "mini batch cost of batch 4000 is : 0.01690651583485305\n",
      "mini batch cost of batch 4020 is : 0.016493782214820386\n",
      "mini batch cost of batch 4040 is : 0.016692474996671078\n",
      "mini batch cost of batch 4060 is : 0.016806299285963174\n",
      "mini batch cost of batch 4080 is : 0.01736229825764895\n",
      "mini batch cost of batch 4100 is : 0.017223272984847427\n",
      "mini batch cost of batch 4120 is : 0.01623527822084725\n",
      "mini batch cost of batch 4140 is : 0.01739894379861653\n",
      "mini batch cost of batch 4160 is : 0.017300762515515086\n",
      "mini batch cost of batch 4180 is : 0.017844523862004278\n",
      "mini batch cost of batch 4200 is : 0.016420345799997446\n",
      "mini batch cost of batch 4220 is : 0.016772986948490144\n",
      "mini batch cost of batch 4240 is : 0.01682595727033913\n",
      "mini batch cost of batch 4260 is : 0.016923381946980953\n",
      "mini batch cost of batch 4280 is : 0.016986137861385944\n",
      "mini batch cost of batch 4300 is : 0.016639589192345736\n",
      "mini batch cost of batch 4320 is : 0.017494618380442264\n",
      "mini batch cost of batch 4340 is : 0.016601677192375066\n",
      "mini batch cost of batch 4360 is : 0.01652324115857482\n",
      "mini batch cost of batch 4380 is : 0.016869820654392242\n",
      "mini batch cost of batch 4400 is : 0.0169777630828321\n",
      "mini batch cost of batch 4420 is : 0.017114470573142172\n",
      "mini batch cost of batch 4440 is : 0.016887530498206616\n",
      "mini batch cost of batch 4460 is : 0.016648414218798282\n",
      "mini batch cost of batch 4480 is : 0.0172559563536197\n",
      "mini batch cost of batch 4500 is : 0.017653792491182683\n",
      "mini batch cost of batch 4520 is : 0.016432163910940288\n",
      "mini batch cost of batch 4540 is : 0.016982984356582163\n",
      "mini batch cost of batch 4560 is : 0.017276523867622018\n",
      "mini batch cost of batch 4580 is : 0.016465545864775777\n",
      "mini batch cost of batch 4600 is : 0.016311769001185895\n",
      "mini batch cost of batch 4620 is : 0.01627947096712887\n",
      "mini batch cost of batch 4640 is : 0.016353761265054344\n",
      "cost after epoch : 0.016567224407789558\n",
      "cost after epoch : 0.03322740518724767\n",
      "cost after epoch : 0.04988995766646024\n",
      "cost after epoch : 0.06653538543604694\n"
     ]
    }
   ],
   "source": [
    "model(learning_rate=.00960955,num_epochs=4,mini_size=16,break_t=10000,break_v=1000,pt_out=20,hole_pera=6.0,valid_pera=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "I82s6e35OH1t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "landsat_image _inpaintingerere.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
